----------------------------------------------------
Part I: Find Machine Characteristics
----------------------------------------------------
What CPU are you using?
Intel(R) Core(TM) i7-2600 CPU (4 cores)

What is the operating frequency of the cores:
Max CPU operating frequency is 3.4 GHz
Current CPU operating frequency is 1.6 GHz

Cache size?
4x32kB Level 1 cache
4x256kB Level 2 cache
8MB Level 3 shared cache

Microarchitecture?
Sandy Bridge

Number of cores?
There are 4 cores


----------------------------------------------------
Part II: Timers
----------------------------------------------------


$$$$$notes
Use -O0 optimization
-lrt needed for clock_gettime()

simplest timing method - shell-level time command
ex:
$time ./test_timers
real 0m0.993s
user 0.0.989s
sys  0m0.002s

Measuring time intervals within code:
times() - elapsed time in ticks
gettimeofday() - elapsed time in us
RDTSC - assembly level rdtsc for cycle count (low overhead, calibration needed)
clock_gettime() - ns resolution high overhead
$$$$$end notes


How do you determin accuracy generally?
 The way you can determine how accurate a timer is, is by starting with reptitive operation that you perform many times. After this, you can take the average of time taken per run. From there, you decrease the number of iterations, and compare the calculated value with the obtained value. Once those numbers start diverging, you know you have reachecd the limit of accuracy. 	

What is the resolution?
Using gettimeofday your resolution can be determined by how few iterations can be done before you don't get a value. From the progression, we can see that if you go below 100 iterations, you will get a time of 0, meaning the maximum resolution of the timer is 1 us.

For RDTSC:
By taking the total time and dividing by the number of iterations, we have the timer per iteration taking between 3.4~3.6ns, until we get to about 10 iterations where the time jumps to 6.5ns. This means that the accuracy of the timer holds until you get to about the tens of ns, after which timing accuracy gets conflicted with other processes.

For times:
We can see that the measurements go to 0 if the timing goes below .02s, making that the limit of the timer.


Q: What problems do RDTSC methods have?
With new multiple CPU chips, there is no promise the timestamp counters on the different CPUs are the same. This may lead to the problem where you are measurering time on on CPU, but get a completely different reading on another CPU.
With power-saving features on the OS, the reading may not be accurate. For example, if the system is hibernated and resumed, you will reset the timer.
Out of order instructions may also cause the timing command to be run at a different time than expected, skewing time measurement.


Can these timers still be useful?
The timer can still be useful if these issues are corrected. For example, the timestamp counter can be synchronized before RDTSC is run. Power-saving features can be turned off in the OS, eliminating this problem. Finally by running a serializing version of RDTSC you can keep track of when instructions are run, eliminating skewing from out-of-order instructions.


What needs to be done for each timer?

gettimeofday
The processes run by the function do some internal calibration during startup. In the code however, the end result of the function is corrected by dividing by GET_TOD_TICS. All this seems to do however, is convert from microseconds to seconds. Since this constant is set to 1e6, which is correct, there is no need to change anything.


RDTSC









































